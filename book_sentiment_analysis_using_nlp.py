# -*- coding: utf-8 -*-
"""BOOK SENTIMENT ANALYSIS USING NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/198uKmfZ5TOKLAjmPJk5oWbW1dNEHaiFl

**Assignment-2**

---
"""

!pip install requests
!pip install beautifulsoup4

# Web scraping, pickle imports
import requests
from bs4 import BeautifulSoup
import pickle

# Scrapes transcript data from scrapsfromtheloft.com
def url_to_transcript(url):
    '''Returns transcript data specifically from scrapsfromtheloft.com.'''
    page = requests.get(url).text
    soup = BeautifulSoup(page, "lxml")
    text = [p.text for p in soup.find(class_="site-content").find_all("p")]
    print(url)
    return text

# URLs of transcripts in scope
urls = ['https://scrapsfromtheloft.com/books/yanis-varoufakis-technofeudalism-american-big-tech-has-enslaved-us/',
        'https://scrapsfromtheloft.com/books/john-gray-everything-you-know-about-the-future-is-wrong/',
        'https://scrapsfromtheloft.com/books/the-fall-of-the-house-of-usher-1839-by-edgar-allan-poe/',
        'https://scrapsfromtheloft.com/books/2021-year-milley-halted-nuclear-chaos/',
        'https://scrapsfromtheloft.com/books/complete-works-of-oscar-wilde-vyvyan-holland-introduction/',
        'https://scrapsfromtheloft.com/books/oscar-wildes-de-profundis-tale-of-art-pain-and-redemption/',
        'https://scrapsfromtheloft.com/books/joan-didion-on-keeping-a-notebook/',
        'https://scrapsfromtheloft.com/books/mark-twain-damned-human-race/',
        'https://scrapsfromtheloft.com/history/war-and-slaves-in-ancient-greece-and-rome-luciano-canfora/',
        'https://scrapsfromtheloft.com/books/fall-of-house-of-usher-allegory-of-artist/',
        'https://scrapsfromtheloft.com/books/mary-mccarthy-on-hannah-arendt-origins-of-totalitarianism/',
        'https://scrapsfromtheloft.com/books/judith-jarvis-thomson-a-defense-of-abortion/']

# Author names
author = ['yanis', 'john', 'edgar', 'luca', 'vyvyan', 'Guido', 'joan', 'mark', 'Luciano', 'Hoffman', 'Hannah', 'Judith']

# Actually request transcripts (takes a few minutes to run)
transcripts = [url_to_transcript(u) for u in urls]

# Make a new directory to hold the text files
!mkdir -p transcripts

# Pickle files for later use
for i, c in enumerate(author):
    with open("transcripts/" + c + ".txt", "wb") as file:
        pickle.dump(transcripts[i], file)

# Load pickled files
data = {}
for i, c in enumerate(author):
    with open("transcripts/" + c + ".txt", "rb") as file:
        data[c] = pickle.load(file)

# Double check to make sure data has been loaded properly
data.keys()

# More checks
data['edgar'][:2]

# Let's take a look at our data again
next(iter(data.keys()))

next(iter(data.values()))

# We are going to change this to key: author, value: string format
def combine_text(list_of_text):
    '''Takes a list of text and combines them into one large chunk of text.'''
    combined_text = ' '.join(list_of_text)
    return combined_text

# Combining it in new dictionay
data_combined = {key: [combine_text(value)] for (key, value) in data.items()}

# We can either keep it in dictionary format or put it into a pandas dataframe (row author , column transcript)
import pandas as pd
pd.set_option('max_colwidth',150)

data_df = pd.DataFrame.from_dict(data_combined).transpose()
data_df.columns = ['transcript']
data_df = data_df.sort_index()
data_df

# Let's take a look at the transcript for Hoffman
data_df.transcript.loc['Hoffman']

# Apply a first round of text cleaning techniques
import re
import string

def clean_text_round1(text):
    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''
    text = text.lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\w*\d\w*', '', text) # word containing numbers
    return text

round1 = lambda x: clean_text_round1(x)

# Let's take a look at the updated text
data_clean = pd.DataFrame(data_df.transcript.apply(round1))
data_clean

# Apply a second round of cleaning
def clean_text_round2(text):
    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''
    text = re.sub('[‘’“”…]', '', text)
    text = re.sub('\n', '', text)
    return text

round2 = lambda x: clean_text_round2(x)

# Let's take a look at the updated text
data_clean = pd.DataFrame(data_clean.transcript.apply(round2))
data_clean

# Let's take a look at our dataframe
data_df

# Let's add the authors full names as well
full_names = ['Guido-bulla', 'Hannah-gillis', 'Hoffman-freid', 'Judith-martin', 'Luciano-chappelle', 'edgar-allan poe',
              'joan-ansari', 'john-didion', 'luca-notaro', 'mark-milley', 'vyvyan-Holland', 'yanis-carlin']
full_names.sort()
data_df['full_name'] = full_names
data_df

# Let's pickle it for later use
data_df.to_pickle("corpus.pkl")

# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

cv = CountVectorizer(stop_words='english')
data_cv = cv.fit_transform(data_clean.transcript)
data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())
data_dtm.index = data_clean.index

# Let's pickle it for later use
data_dtm.to_pickle("dtm.pkl")

# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object
data_clean.to_pickle('data_clean.pkl')
pickle.dump(cv, open("cv.pkl", "wb"))

"""Additional"""

print("Before Second Round of Cleaning:")
print(data_clean['transcript'].head())

#1  second round of cleaning
def clean_text_round2(text):
    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''
    text = re.sub('[‘’“”…]', '', text)
    text = re.sub('\n', '', text)

    # Additional regular expression to remove URLs
    text = re.sub(r'http\S+', '', text)
    text = re.sub(' +', ' ', text)  # Replacing multiple spaces with a single space



    return text

round2 = lambda x: clean_text_round2(x)

data_clean = pd.DataFrame(data_clean.transcript.apply(round2))

data_clean

import nltk   ##Natural Language Toolkit
from nltk.corpus import stopwords
nltk.download('punkt')

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def clean_text_round3(text):
    '''Remove stopwords and perform additional cleaning in the third round.'''
    # Tokenize the text
    words = nltk.word_tokenize(text)

    # Remove stopwords
    words = [word for word in words if word.lower() not in stop_words]

    # Join the words back into a string
    text = ' '.join(words)



    return text

round3 = lambda x: clean_text_round3(x)

data_clean = pd.DataFrame(data_clean.transcript.apply(round3))

data_clean

#2
from sklearn.feature_extraction.text import CountVectorizer

#  max_features
cv_max_features = CountVectorizer(stop_words='english', max_features=1000)
data_cv_max_features = cv_max_features.fit_transform(data_clean.transcript)
data_dtm_max_features = pd.DataFrame(data_cv_max_features.toarray(), columns=cv_max_features.get_feature_names_out())
data_dtm_max_features.index = data_clean.index
print(data_dtm_max_features)

# max_df and min_df
cv_df_thresholds = CountVectorizer(stop_words='english', max_df=0.85, min_df=0.1)
data_cv_df_thresholds = cv_df_thresholds.fit_transform(data_clean.transcript)
data_dtm_df_thresholds = pd.DataFrame(data_cv_df_thresholds.toarray(), columns=cv_df_thresholds.get_feature_names_out())
data_dtm_df_thresholds.index = data_clean.index
print(data_dtm_df_thresholds)

#ngram_range
cv_ngram = CountVectorizer(stop_words='english', ngram_range=(1, 2))
data_cv_ngram = cv_ngram.fit_transform(data_clean.transcript)
data_dtm_ngram = pd.DataFrame(data_cv_ngram.toarray(), columns=cv_ngram.get_feature_names_out())
data_dtm_ngram.index = data_clean.index
print(data_dtm_ngram)

"""**ASSIGNMENT-3**

---

**EXPLORATORY DATA ANALYSIS**

---
"""

dtm_file_path = "dtm.pkl"

# Read in the document-term matrix from the pickle file
data_dtm = pd.read_pickle(dtm_file_path)

data_dtm

# Find and print the top 30 words
# Sum the counts of each word across all documents
word_counts = data_dtm.sum()

# Sort the words based on their total counts in descending order
sorted_word_counts = word_counts.sort_values(ascending=False)

# Print the top 30 words
top_30_words = sorted_word_counts.head(30)
print(top_30_words)

# By looking at these top words, you can see that some of them have very little
# meaning and could be added to a stop words list, so let's do just that.
# Look at the most common top words and add them to the stop word list.


# List of the most common top words
common_top_words = sorted_word_counts.head(30).index.tolist()

# Print the common top words
print("Common Top Words:")
print(common_top_words)


from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
custom_stop_words = list(ENGLISH_STOP_WORDS) + common_top_words

# Printing the custom stop words
print("\nCustom Stop Words:")
print(custom_stop_words)

# Let's aggregate this list and identify the most common words along with how many routines they occur in

# Create a new DataFrame with word counts aggregated across routines
word_occurrences = (data_dtm > 0).sum(axis=0)
#data_dtm is converted from dtm to binary to check if there is occurence of the word across any of the transcript , if yes then true and count =1 , axis =0 to check across all transcripts.

# Sort the words based on their total occurrences in descending order
sorted_word_occurrences = word_occurrences.sort_values(ascending=False)

# Print the top words and their occurrences
top_words_with_occurrences = sorted_word_occurrences.head(30)
print(top_words_with_occurrences)

# Create a new DataFrame with word counts aggregated across routines
word_occurrences = (data_dtm > 0).sum(axis=0)

# Filter out words that are top words for more than half of the authors
half_authors_count = len(data_dtm) / 2   #determining the threshold for whether the word is a top word or not .
filtered_word_occurrences = word_occurrences[word_occurrences <= half_authors_count] #select words less than the threshold

# Sort the remaining words based on their total occurrences in descending order
sorted_filtered_word_occurrences = filtered_word_occurrences.sort_values(ascending=False)

# Print the top words and their occurrences
top_words_with_occurrences = sorted_filtered_word_occurrences.head(30)
print(top_words_with_occurrences)

# Read in cleaned data (replace 'data_clean.pkl' with the actual path to your cleaned data pickle file)
data_clean = pd.read_pickle('data_clean.pkl')

# Add new stop words to an existing stop words list (e.g., scikit-learn's ENGLISH_STOP_WORDS)
stop_words = list(ENGLISH_STOP_WORDS) + custom_stop_words

# Recreate document-term matrix with the updated stop words
cv = CountVectorizer(stop_words=stop_words) #CountVectorizer is used to convert a collection of text documents into a matrix of token counts.
data_cv = cv.fit_transform(data_clean.transcript)
data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())
data_dtm.index = data_clean.index

# Pickle the updated document-term matrix for later use
data_dtm.to_pickle("updated_dtm.pkl")

# Let's make some word clouds!


!pip install wordcloud

# Import necessary libraries
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Assuming 'data_dtm' is your document-term matrix DataFrame
data_dtm = pd.read_pickle('updated_dtm.pkl')

# Concatenate all transcripts into a single string
all_transcripts = ' '.join(data_clean.transcript) #This is necessary because the WordCloud library expects a single string input.

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=400, random_state=10, max_font_size=110, background_color='white').generate(all_transcripts)

# Plot the WordCloud image
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# Assuming 'data_clean' is your cleaned data DataFrame
data_clean = pd.read_pickle('data_clean.pkl')

# Import necessary libraries
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Set the number of rows and columns for subplots
num_rows = 4
num_cols = 3

# Create subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 15))

# Iterate through authors and create word clouds
for i, author in enumerate(data_clean.index):
    row = i // num_cols
    col = i % num_cols

    # Tokenize the transcript for the current author
    authors_transcript = data_clean.loc[author, 'transcript'].split()  # Assuming transcripts are whitespace-separated

    # Create a WordCloud object
    wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, background_color='white').generate(' '.join(authors_transcript))

    # Plot the WordCloud image in the corresponding subplot
    axes[row, col].imshow(wordcloud, interpolation="bilinear")
    axes[row, col].axis('off')
    axes[row, col].set_title(author)

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""**YOUR OBSERVATION**

---



There are some common top words
right

*   life
*   economic
*   man
*   digital
*   person
*   house

**ASSIGNMENT-2**

---
"""

#1 Find the number of unique words that each author uses

# Count the number of unique words for each author
unique_words_count = data_dtm.apply(lambda row: len(row[row > 0]), axis=1) #The lambda function counts the number of elements in each row where the value is greater than 0, rowwise

# Create a new column in the DataFrame to store the count of unique words
data_df['unique_words_count'] = unique_words_count

# Display the DataFrame with author names and their corresponding unique word counts
print(data_df[['full_name', 'unique_words_count']])

#this code snippet provides a detailed view of the non-zero entries in the document-term matrix, indicating which words appear in the transcripts of which authors and their respective counts.
import numpy as np

# Identify non-zero items in the document-term matrix
nonzero_indices = np.nonzero(data_dtm.values)

# Extract row and column indices
row_indices, col_indices = nonzero_indices

# Create a DataFrame to display the results
nonzero_items_df = pd.DataFrame({
    'Author': data_dtm.index[row_indices],
    'Word': data_dtm.columns[col_indices],
    'Count': data_dtm.values[row_indices, col_indices]
})

# Display the DataFrame with non-zero items
print(nonzero_items_df)

# Create a new DataFrame for unique word count
unique_words_count_df = pd.DataFrame({
    'Author': data_df['full_name'],
    'Unique_Word_Count': data_df['unique_words_count']
})

# Display the new DataFrame
print(unique_words_count_df)

# Calculate total words in each comedian transcript
total_words_per_author = data_dtm.sum(axis=1)

# Create a new DataFrame for total words
total_words_df = pd.DataFrame({
    'Author': data_df['full_name'],
    'Total_Words': total_words_per_author
})

# Display the new DataFrame with total words
print(total_words_df)

# Example data (replace with your actual data)
total_words = [1056,365,1773,2811,433,3203,1188,2319,704,1471,1350,2100]  # Replace with the actual total words for each author
total_pages = [15,10,20,34,12,17,16,13,14,15,15,16]  # Replace with the total pages of each book

# Create a new DataFrame for words per minute
wpp_df = pd.DataFrame({
    'Author': data_df['full_name'],
    'Total_Words': total_words,
    'Total_Pages': total_pages
})

# Calculate words per minute
wpp_df['Words_Per_Pge'] = wpp_df['Total_Words'] / wpp_df['Total_Pages']

# Display the new DataFrame with words per minute
print(wpp_df)

# Add 'total_words' column to the existing DataFrame
data_df['total_words'] = total_words_df['Total_Words']

# Add 'total_pages' column (replace with your actual pages)
data_df['total_pages'] =[15,10,20,34,12,17,16,13,14,15,15,16]
# Calculate 'words per page'
data_df['words_per_book'] = data_df['total_words'] / data_df['total_pages']

# Display the updated DataFrame
print(data_df[['full_name', 'total_words', 'total_pages', 'words_per_book']])

# Sort the DataFrame by 'words_per_minute' in ascending order
sorted_df = data_df.sort_values(by='words_per_book')

# Display the sorted DataFrame
print(sorted_df[['full_name', 'words_per_book']])

import matplotlib.pyplot as plt

# Sort the DataFrame by 'words_per_page' in ascending order
sorted_df = data_df.sort_values(by='words_per_book')

# Plot the bar chart
plt.figure(figsize=(12, 6))
plt.bar(sorted_df['full_name'], sorted_df['words_per_book'], color='skyblue')
plt.xlabel('Author')
plt.ylabel('Words Per Page')
plt.title('Words Per Page for Each Author')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

#this code snippet creates a bar chart visualizing the words per page for each author, allowing for easy comparison of writing density among authors.
#understand writing habits , style,literary analysis , reader expectation, editorial consideration

"""**YOUR OBSERVATION**

---
. John-didion and edgar-allan poe use a lot of words in their book

#Using unique and meaningful words

· john-didion and edgar-allan poe uses more unique words

. Luciano-chappelle and Hannah-gilis uses less meaningful words

. vyvyan-holland and hoffman-freid are in middle

**ASSIGNMENT-3**

---
"""

# Calculate the total word frequencies across all authors
total_word_frequencies = data_dtm.sum()

# Create a DataFrame for the most common words
common_words_df = pd.DataFrame({
    'Word': total_word_frequencies.index,
    'Frequency': total_word_frequencies.values
})

# Sort the DataFrame by frequency in descending order
common_words_df = common_words_df.sort_values(by='Frequency', ascending=False)

# Plot the bar chart
plt.figure(figsize=(12, 6))
plt.bar(common_words_df['Word'][:15], common_words_df['Frequency'][:15], color='skyblue')
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.title('Top 15 Most Common Words Across All Authors')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()
#theme identification, writing style analysis ,content overview ,

# List of positive and negative words for comparison
positive_words = ['happy', 'joy', 'love', 'fun']
negative_words = ['sad', 'angry', 'hate', 'stress']

# Filter words that exist in the DTM columns
existing_positive_words = [word for word in positive_words if word in data_dtm.columns]
existing_negative_words = [word for word in negative_words if word in data_dtm.columns]

# Select relevant columns from the document-term matrix
positive_occurrences = data_dtm[existing_positive_words].sum(axis=1)
negative_occurrences = data_dtm[existing_negative_words].sum(axis=1)

# Create a DataFrame for word occurrences
word_comparison_df = pd.DataFrame({
    'Author': data_df['full_name'],
    'Positive_Occurrences': positive_occurrences,
    'Negative_Occurrences': negative_occurrences
})

# Plot the line plot
plt.figure(figsize=(10, 6))
plt.plot(word_comparison_df['Positive_Occurrences'], label='Positive Words', marker='o')
plt.plot(word_comparison_df['Negative_Occurrences'], label='Negative Words', marker='o')
plt.xlabel('Author')
plt.ylabel('Word Occurrences')
plt.title('Comparison of Positive and Negative Word Occurrences')
plt.legend()
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

#sentiment /tone of writing , theme , character , narrative stratigies, genre /content analysis
#positive - happiness,joy,optimism
#negative-anger,sad,conflict

"""# Topic Modeling
## Introduction
Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.

In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.

To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.

Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model.

## Topic Modeling - Attempt #1 (All Text)
"""

import pandas as pd
import pickle

data = pd.read_pickle('data_clean.pkl')
data

from sklearn.feature_extraction.text import CountVectorizer

# remove stop words from the data
cv = CountVectorizer(stop_words='english', min_df=3, max_df=0.8)

# Fit and transform the 'transcript' column of the DataFrame
dtm = cv.fit_transform(data['transcript'])

# Convert the term-document matrix to a DataFrame
dtm_df = pd.DataFrame(dtm.toarray(), columns=cv.get_feature_names_out())

# Set the index of the DataFrame to match the index of the original DataFrame
dtm_df.index = data.index

# Print the resulting term-document matrix DataFrame
print(dtm_df)

# Save the term-document matrix DataFrame to a pickle file
dtm_df.to_pickle('dtm.pkl')

# Save the CountVectorizer object to a pickle file
with open('cv.pkl', 'wb') as f:
    pickle.dump(cv, f)

# Let's read in our document-term matrix
import pandas as pd
import pickle

data = pd.read_pickle('dtm.pkl')
data

# Import the necessary modules for LDA with gensim

from gensim import matutils, models
import scipy.sparse

# One of the required inputs is a term-document matrix
tdm = data.transpose()
tdm.head()

# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus
sparse_counts = scipy.sparse.csr_matrix(tdm)  # compressed sparse rows - good for non zero elements
corpus = matutils.Sparse2Corpus(sparse_counts)  # convert to gensim corpus - need for i/p to gensim modelling algo
print(corpus)

# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix

cv = pickle.load(open("cv.pkl", "rb"))
id2word = dict((v, k) for k, v in cv.vocabulary_.items())
print(id2word)

# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),
# we need to specify two other parameters as well - the number of topics and the number of passes
lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)
lda.print_topics()

# LDA for num_topics = 3
lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)
lda.print_topics()

# LDA for num_topics = 4
lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)
lda.print_topics()

"""These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well.

## Topic Modeling - Attempt #2 (Nouns Only)

One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.).
"""

import nltk
nltk.download('averaged_perceptron_tagger')  # need for parts of speech tagging

# Let's create a function to pull out nouns from a string of text
from nltk import word_tokenize, pos_tag

def nouns(text):
    '''Given a string of text, tokenize the text and pull out only the nouns.'''
    is_noun = lambda pos: pos[:2] == 'NN' # returns True if pos is noun
    tokenized = word_tokenize(text)
    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)]
    return ' '.join(all_nouns)

# Read in the cleaned data, before the CountVectorizer step
data_clean = pd.read_pickle('data_clean.pkl')
data_clean

# Apply the nouns function to the transcripts to filter only on nouns
nltk.download('punkt')
data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))
data_nouns

# Create a new document-term matrix using only nouns
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import CountVectorizer
# Recreate a document-term matrix with only nouns
cvn = CountVectorizer(stop_words=list(stop_words))
data_cvn = cvn.fit_transform(data_nouns.transcript)
data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names_out())
data_dtmn.index = data_nouns.index
data_dtmn

# Create the gensim corpus
corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))

# Create the vocabulary dictionary
id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())
print(id2wordn)

# Let's start with 2 topics
ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)
ldan.print_topics()

# Let's try topics = 3
ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)
ldan.print_topics()

# Let's try 4 topics
ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)
ldan.print_topics()

"""## Topic Modeling - Attempt #3 (Nouns and Adjectives)"""

# Let's create a function to pull out nouns from a string of text
def nouns_adj(text):
    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''
    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'
    tokenized = word_tokenize(text)
    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)]
    return ' '.join(nouns_adj)

# Apply the nouns function to the transcripts to filter only on nouns
data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))
data_nouns_adj

# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df
cvna = CountVectorizer(stop_words=list(stop_words), max_df=.8)
data_cvna = cvna.fit_transform(data_nouns_adj.transcript)
data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())
data_dtmna.index = data_nouns_adj.index
data_dtmna

# Create the gensim corpus
corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))

# Create the vocabulary dictionary
id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())

# Let's start with 2 topics
ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)
ldana.print_topics()

# Let's try 3 topics
ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)
ldana.print_topics()

# Let's try 4 topics
ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)
ldana.print_topics()

"""## Identify Topics in Each Document"""

# Our final LDA model (for now)
ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)
ldana.print_topics()

"""These four topics look pretty decent. Let's settle on these for now.
* Topic 0 seems to be related to literature and history, with words like "book", "ancient", "rome", etc.
* Topic 1 appears to be about literature and authors, with words like "wilde", "oscar", "novel", etc.
* Topic 2 seems to be related to economic and political issues, with words like "economic", "social", "political", etc.
* Topic 3 appears to be about personal and ethical topics, with words like "person", "abortion", "body", etc
"""

# Let's take a look at which topics each transcript contains
corpus_transformed = ldana[corpusna]
list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))

"""For a first pass of LDA, these kind of make sense to me, so let's settle on these for now
* Topic 0: Literature & history [Hannah,Luciano]
* Topic 1: Literature & authors [Guido,Edgar,Luca,Mark,Vyvyan]
* Topic 2: Economic & political issues [John,Yanis]
* Topic 3: Personal & ethical topics [Hoffman,Judith,Joan]

### Assignment:
1. Try further modifying the parameters of the topic models above and see if you can get better topics.
2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics.
"""

#Part 1
ldana = models.LdaModel(corpus=corpusna, num_topics=6, id2word=id2wordna, passes=50)
ldana.print_topics()

"""Topic 0 :This topic appears to focus on personal and ethical issues, with words like "person", "abortion", "man", "house", "body", "death".

Topic 1:This topic seems to revolve around historical and literary themes, featuring terms like "rome", "war", "ancient", "slaves", "roman", "slavery".

Topic 2:This topic could be related to literature and character analysis, with terms like "usher", "house", "long", "certain", "character", "words", "wildes", "family", "portion".

Topic 3:Economic and political issues seem to be central here, with terms such as "economic", "social", "political", "gray", "grays", "policies", "future", "critique", "crisis", "global".

Topic 4:This topic may focus on digital and technological aspects, featuring terms like "digital", "economic", "varoufakis", "power", "tech", "technofeudalism", "data", "platforms", "financial", "cloud".

Topic 5:This topic appears to be related to military and geopolitical matters, with words like "trump", "military", "president", "milley", "nuclear", "january", "general", "chinese", "joint", "trumps".
"""

#Part 2
# Create a function to extract verbs from text
def verbs(text):
    '''Given a string of text, tokenize the text and pull out only the verbs.'''
    is_verb = lambda pos: pos[:2] == 'VB'
    tokenized = word_tokenize(text)
    all_verbs = [word for (word, pos) in pos_tag(tokenized) if is_verb(pos)]
    return ' '.join(all_verbs)

# Apply the verbs function to the transcripts to filter only on verbs
data_verbs = pd.DataFrame(data_clean.transcript.apply(verbs))

# Create a new document-term matrix using only verbs
cv_verbs = CountVectorizer(stop_words=list(stop_words))
data_cv_verbs = cv_verbs.fit_transform(data_verbs.transcript)
data_dtm_verbs = pd.DataFrame(data_cv_verbs.toarray(), columns=cv_verbs.get_feature_names_out())
data_dtm_verbs.index = data_verbs.index

# Create the gensim corpus for verbs
corpus_verbs = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtm_verbs.transpose()))

# Create the vocabulary dictionary for verbs
id2word_verbs = dict((v, k) for k, v in cv_verbs.vocabulary_.items())

# Attempt LDA with verbs
lda_verbs = models.LdaModel(corpus=corpus_verbs, num_topics=4, id2word=id2word_verbs, passes=10)
lda_verbs.print_topics()

"""# Text Generation
## Introduction
Markov chains can be used for very basic text generation. Think about every word in a corpus as a state. We can make a simple assumption that the next word is only dependent on the previous word - which is the basic assumption of a Markov chain.

Markov chains don't generate text as well as deep learning, but it's a good (and fun!) start.
## Select Text to Imitate
In this notebook, we're specifically going to generate text in the style of Guido so as a first step, let's extract the text from his transcript.

"""

import pandas as pd

data = pd.read_pickle('corpus.pkl')
data

guido_text = data.transcript.loc['Guido']
guido_text[:200] # first 200 characters

"""## Build a Markov Chain Function
We are going to build a simple Markov chain function that creates a dictionary:
* The keys should be all of the words in the corpus
* The values should be a list of the words that follow the keys
"""

from collections import defaultdict

def markov_chain(text):
    words = text.split(' ')
    m_dict = defaultdict(list)
    for current_word, next_word in zip(words[0:-1], words[1:]):
        m_dict[current_word].append(next_word)
    m_dict = dict(m_dict)
    return m_dict

guido_dict = markov_chain(guido_text)
guido_dict

"""## Create a Text Generator
We're going to create a function that generates sentences. It will take two things as inputs:
* The dictionary you just created
* The number of words you want generated


"""

import random

def generate_sentence(chain, count=15):
    '''Input a dictionary in the format of key = current word, value = list of next words
       along with the number of words you would like to see in your generated sentence.'''


    word1 = random.choice(list(chain.keys()))
    sentence = word1.capitalize()

    for i in range(count-1):
        word2 = random.choice(chain[word1])
        word1 = word2
        sentence += ' ' + word2


    sentence += '.'
    return(sentence)

generate_sentence(guido_dict)

import random
import string

def generate_sentence_with_punctuation(chain, max_length=15):
    '''Generate a sentence with the given Markov chain, ending with a random punctuation mark.'''
    word1 = random.choice(list(chain.keys()))
    sentence = [word1.capitalize()]

    while len(sentence) < max_length:
        word2 = random.choice(chain.get(word1, ['']))
        if not word2:
            break
        sentence.append(word2)
        if word2[-1] in string.punctuation:
            break
        word1 = word2

    # Add a random punctuation mark if the sentence doesn't end with one
    if sentence[-1][-1] not in string.punctuation:
        sentence[-1] += random.choice(string.punctuation)

    return ' '.join(sentence)

generate_sentence_with_punctuation(guido_dict)

from collections import defaultdict

def n_gram_model(text, n=2):
    '''Generate an N-gram model from the input text.'''
    words = text.split()
    n_grams = defaultdict(list)
    for i in range(len(words) - n + 1):
        n_gram = tuple(words[i:i + n])
        next_word = words[i + n] if i + n < len(words) else None
        n_grams[n_gram].append(next_word)
    return dict(n_grams)

n_gram_model(guido_text)

import re

def tokenize_text_advanced(text):
    '''Tokenize the input text using advanced tokenization techniques.'''
    text = re.sub(r'[^\w\s\'-]', '', text)
    words = re.findall(r"\b\w+(?:[-']\w+)*\b", text)
    return words

tokenize_text_advanced(guido_text)

from collections import defaultdict

def laplace_smoothing(text, n=1):
    '''Generate a Markov chain model with Laplace smoothing.''' # add a small count to ensure no probability is zero.
    words = text.split()
    n_grams = defaultdict(lambda: defaultdict(int))
    for i in range(len(words) - n):
        n_gram = tuple(words[i:i + n])
        next_word = words[i + n]
        n_grams[n_gram][next_word] += 1

    # Apply Laplace smoothing
    vocabulary_size = len(set(words))
    for n_gram in n_grams:
        total_count = sum(n_grams[n_gram].values())
        for word in n_grams[n_gram]:
            n_grams[n_gram][word] = (n_grams[n_gram][word] + 1) / (total_count + vocabulary_size)

    return dict(n_grams)

laplace_smoothing(guido_text)